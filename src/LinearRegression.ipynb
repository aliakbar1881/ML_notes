{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1171436d-ea4f-4d8b-917e-9465b62c1465",
   "metadata": {},
   "source": [
    "# LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e2452a-8594-4d8b-a0ee-7808912f9336",
   "metadata": {},
   "source": [
    "Linear regression analysis is used to predict the value of a variable based on the value of another variable. The variable you want to predict is called the dependent variable. The variable you are using to predict the other variable's value is called the independent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7a2e8c-12ed-457f-b173-830841adccfd",
   "metadata": {},
   "source": [
    "<u>*supervised learning:*\n",
    "*given the \"right ansewr\" for each example in the data.*</u>\n",
    "\n",
    "<u>*regression problem:*\n",
    "*predict real-valued output*</u>\n",
    "\n",
    "<u>*calssification:*\n",
    "*Discrete-valued otput**</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590604b0-70d8-4051-b7af-f7c50b08420d",
   "metadata": {},
   "source": [
    "<font color='green'>Notation:</font>\n",
    "\n",
    "- `m`: Number of trainig examples\n",
    "- `x`: \"input\" variavle/ features\n",
    "- `y`: \"output\" varibale / \"target\" variable\n",
    "- `(x, y)`: one traning example\n",
    "- `(x^(i), y^(i))`: i*th* traning example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f4883b-fca4-4cc2-b060-d1d30865125f",
   "metadata": {},
   "source": [
    "to solve LinearRegression:\n",
    " 1. Trainig Set\n",
    " 2. Leeraning algorithm by machine\n",
    " 3. Hypothesised function\n",
    "\n",
    "<img src=\"./images/0.png\">\n",
    "\n",
    "<font color=\"red\">For housing prices we have a Training set of houses size and price corresponding to size feature then we find the best line that fit to data and called the hyphotesised function. If I hane a new house thus send size of my house to hypothesised function as result I can have that price.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808c43c2-6e17-4524-a2fa-92d343bcf9e9",
   "metadata": {},
   "source": [
    "### Linear Regression to Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fd600a-0b03-41c0-9978-5c4f10ac268d",
   "metadata": {},
   "source": [
    "Learning a linear regression model means estimating the values of the coefficients used in the representation with the data that we have available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0523d38-21f0-4f23-af2f-66f4fc0dda34",
   "metadata": {},
   "source": [
    "There are many more techniques because the model is so well studied. Take note of Ordinary Least Squares because it is the most common method used in general. Also take note of Gradient Descent as it is the most common technique taught in machine learning classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc3969e-e0b0-4fde-a723-4c8561438162",
   "metadata": {},
   "source": [
    "1. **Simple Linear Regression** : With simple linear regression when we have a single input, we can use statistics to estimate the coefficients. This requires that you calculate statistical properties from the data such as means, standard deviations, correlations and covariance. All of the data must be available to traverse and calculate statistics.\n",
    "\n",
    "2. **Ordinary Least Squares** : When we have more than one input we can use Ordinary Least Squares to estimate the values of the coefficients. The Ordinary Least Squares procedure seeks to minimize the sum of the squared residuals. This means that given a regression line through the data we calculate the distance from each data point to the regression line, square it, and sum all of the squared errors together. This is the quantity that ordinary least squares seeks to minimize. This approach treats the data as a matrix and uses linear algebra operations to estimate the optimal values for the coefficients. It means that all of the data must be available and you must have enough memory to fit the data and perform matrix operations. It is unusual to implement the Ordinary Least Squares procedure yourself unless as an exercise in linear algebra. It is more likely that you will call a procedure in a linear algebra library. This procedure is very fast to calculate.\n",
    "\n",
    "3. **Gradient Descent** : When there are one or more inputs you can use a process of optimizing the values of the coefficients by iteratively minimizing the error of the model on your training data. This operation is called Gradient Descent and works by starting with random values for each coefficient. The sum of the squared errors are calculated for each pair of input and output values. A learning rate is used as a scale factor and the coefficients are updated in the direction towards minimizing the error. The process is repeated until a minimum sum squared error is achieved or no further improvement is possible. When using this method, you must select a learning rate (alpha) parameter that determines the size of the improvement step to take on each iteration of the procedure. Gradient descent is often taught using a linear regression model because it is relatively straightforward to understand. In practice, it is useful when you have a very large dataset either in the number of rows or the number of columns that may not fit into memory.\n",
    "\n",
    "4. **Noraml equaion**: The normal equation is a closed-form solution used to find the value of θ that minimizes the cost function. Another way to describe the normal equation is as a one-step algorithm used to analytically find the coefficients that minimize the loss function.\n",
    "\n",
    "## Ordinary Leest Squares (OLS Regression Problem)\n",
    "### Problem Formulation\n",
    "\n",
    "When implementing linear regression of some dependent variable 𝑦 on the set of independent variables 𝐱 = (𝑥₁, …, 𝑥ᵣ), where 𝑟 is the number of predictors, you assume a linear relationship between 𝑦 and 𝐱: 𝑦 = 𝛽₀ + 𝛽₁𝑥₁ + ⋯ + 𝛽ᵣ𝑥ᵣ + 𝜀. This equation is the regression equation. 𝛽₀, 𝛽₁, …, 𝛽ᵣ are the regression coefficients, and 𝜀 is the random error.\n",
    "\n",
    "Linear regression calculates the estimators of the regression coefficients or simply the predicted weights, denoted with 𝑏₀, 𝑏₁, …, 𝑏ᵣ. These estimators define the **estimated regression function 𝑓(𝐱) = 𝑏₀ + 𝑏₁𝑥₁ + ⋯ + 𝑏ᵣ𝑥ᵣ**. This function should capture the dependencies between the inputs and output sufficiently well.\n",
    "\n",
    "The estimated or predicted response, 𝑓(𝐱ᵢ), for each observation 𝑖 = 1, …, 𝑛, should be as close as possible to the corresponding actual response 𝑦ᵢ. **The differences 𝑦ᵢ - 𝑓(𝐱ᵢ) for all observations 𝑖 = 1, …, 𝑛, are called the residuals**. Regression is about determining the best predicted weights—that is, the weights corresponding to the smallest residuals.\n",
    "\n",
    "To get the best weights, you usually minimize the sum of squared residuals (SSR) for all observations 𝑖 = 1, …, 𝑛: SSR = Σᵢ(𝑦ᵢ - 𝑓(𝐱ᵢ))². This approach is called the **method of ordinary least squares**.\n",
    "\n",
    ">The **coefficient of determination**, denoted as 𝑅², tells you which amount of variation in 𝑦 can be explained by the dependence on 𝐱, using the particular regression model. A larger 𝑅² indicates a better fit and means that the model can better explain the variation of the output with different inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed23f71-eedc-4036-bd99-08f594de0f9b",
   "metadata": {},
   "source": [
    "`trick:` The value 𝑅² = 1 corresponds to SSR = 0. That’s the perfect fit, since the values of predicted and actual responses fit completely to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656daae0-9086-4ad5-bec6-5e3b91f6dcaf",
   "metadata": {},
   "source": [
    "Now to sove OLS Rregression problem we have two ways:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f65b17-1c5e-4c71-ad41-04ded3f62dac",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "### Cost Fucntion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77f8a44-c42f-4852-8b03-d2aac14580a8",
   "metadata": {},
   "source": [
    "Now we have folowing detail:  \n",
    "- Training Set\n",
    "\n",
    "- Hypotheasis: Hteta(x) = teta0 + teta1 x\n",
    "    > Hint : teta*i*: parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452e3f15-359e-42bf-b3e9-327f3554d300",
   "metadata": {},
   "source": [
    "How to choose parameter?\n",
    "\n",
    "We want parameter that depict hypothesis fucntion closely by \"true values\" or y. In machine learning we define cost function that imply loss in our fitting. Cost Fucntion in Linear Regression for tow parameter corresponded to:\n",
    "\n",
    "<img src=\"./images/Linear_Regression_Cost_function.png\">\n",
    "\n",
    "If cost function is mimnimized we can find a line that fited to data and closed to \"true values\"\n",
    "\n",
    "> Hint :Sometimes Cost Function called squared error function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cb4fe2-b977-4106-bf7e-db4a9dbf2afb",
   "metadata": {},
   "source": [
    "```suedo\n",
    "repeat until convergence{\n",
    "    tetai := tetai - alpha d/dtetaiJ(teta1, teta0)\n",
    "   } / i=[1, 0]\n",
    "```\n",
    "\n",
    "**Hypothesis**: Hteta = teta0 + teta1 X\n",
    "\n",
    "**parameter**: teta1, teta0\n",
    "\n",
    "**Cost Function**: J(teta1, teta2) = 1/2m SS( Hteta(x^(i)) - Y^(i) ) ** 2\n",
    "\n",
    "**goal**: minimize J(teta1, teta1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dde4ca0-677e-40e3-b5f9-ffcfdb8a0402",
   "metadata": {},
   "source": [
    "### Gradient Descent Algorithm in detail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57d49b5-d601-450f-ae9b-a4e5944577d0",
   "metadata": {},
   "source": [
    "In mathematics, gradient descent (also often called steepest descent) is a first-order iterative optimization algorithm for finding a local minimum of a differentiable function. The idea is to take repeated steps in the opposite direction of the gradient (or approximate gradient) of the function at the current point, because this is the direction of steepest descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99591ea-195b-4f65-977c-0a7abf68d3b6",
   "metadata": {},
   "source": [
    ">alpha: learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca7b32c-5d41-4fe5-85ad-cbddd323e750",
   "metadata": {},
   "source": [
    "Since using a step size *alpha*  that is too small would slow convergence, and a *alpha*  too large would lead to divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1258e3f-61cb-4ff9-954f-bc6bff8b8e85",
   "metadata": {},
   "source": [
    "[to more read](https://en.wikipedia.org/wiki/Gradient_descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf1c279-cde3-43e8-b76e-0445c7946a08",
   "metadata": {},
   "source": [
    "<img src=\"images/Gradient_descent.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ab8319-ca74-4be9-9811-8b9b96328129",
   "metadata": {},
   "source": [
    "```suedo\n",
    "repeat until convergence{\n",
    "    tetai := tetai - alpha d/dtetaiJ(teta1, teta0)\n",
    "   } / i=[1, 0]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbdc370-d145-46a2-aad2-c39fd471e180",
   "metadata": {},
   "source": [
    "Now we can minimize Linear Regression Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cfc55b-06db-4c7e-9ba4-bb26544e0d1f",
   "metadata": {},
   "source": [
    "**“Batch” Gradient Descent**:\n",
    " Each step of gradient descent uses all the training examples.\n",
    " \n",
    " **\"Stochastic\" Gradient Descent** :Stochastic gradient descent is an optimization algorithm often used in machine learning applications to find the model parameters that correspond to the best fit between predicted and actual outputs. It’s an inexact but powerful technique. Stochastic gradient descent is widely used in machine learning applications. Combined with backpropagation, it’s dominant in neural network training applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216c1d7a-f640-47f8-8cff-fb4e8071c11c",
   "metadata": {},
   "source": [
    "## Normal Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65e8317-d4ab-4e27-b882-57d1e1b67194",
   "metadata": {},
   "source": [
    "This is a technique for computing coefficients for Multivariate Linear Regression.\n",
    "the problem is also called OLS Regression, and Normal Equation is an approach of solving it\n",
    "It finds the regression coefficients analytically. It's an one-step learning algorithm (as opposed to Gradient Descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f96062-5ea6-4685-839c-7a41addce142",
   "metadata": {},
   "source": [
    "<img src='./images/Normal_eq.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdbdeaf-63c9-46e0-9d0e-d7f1963ca646",
   "metadata": {},
   "source": [
    "Normal Equation method somtimes encouter to invertibility and non inversible in this situation we use this solutions:\n",
    "1. Remove redundatn features:\n",
    "        for example: x1 is size of house in feet and x2 is size in m\n",
    "        \n",
    "2. Too many features(m<n):\n",
    "        remove unusable features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb3386a-8bb2-4e51-a242-93ad346662b1",
   "metadata": {},
   "source": [
    "[1. Proof of Normal Equation](http://mlwiki.org/index.php/Normal_Equation)\n",
    "\n",
    "[2. Proof of Normal Equation](http://www.jtrive.com/derivation-of-the-normal-equations.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cd5d1e-555a-42e3-8710-a8b1ec613236",
   "metadata": {},
   "source": [
    "### The Normal Equation vs Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5299c6f-65a4-4a5f-98b4-2f4030224d6c",
   "metadata": {},
   "source": [
    "Normal Equation vs Gradient Descent\n",
    "\n",
    "Gradient Descent:\n",
    "\n",
    "- need to choose learning rate α\n",
    "- need to do many iterations\n",
    "- works well with large n\n",
    "\n",
    "\n",
    "Normal Equation:\n",
    "\n",
    "- don't need to choose α\n",
    "- don't need to iterate - computed in one step\n",
    "- slow if n is large (n⩾104)\n",
    "- need to compute (XTX)−1 - very slow\n",
    "- if (XTX) is not-invertible - we have problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2916c35-6131-40e3-b830-e1a1c9559a63",
   "metadata": {},
   "source": [
    "thus in large number of *m* use gradient descent but in small features using Normal eqaution tecnique is well advised"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad26aeae-3e8a-4b3b-8aed-58fabbce1d4b",
   "metadata": {},
   "source": [
    "### Simple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb01bf7-1b59-470e-a21a-b19048b63d71",
   "metadata": {},
   "source": [
    "Simple or single-variate linear regression is the simplest case of linear regression, as it has a single independent variable, 𝐱 = 𝑥.\n",
    "In the 𝑓(𝑥) = 𝑏₀ + 𝑏₁𝑥 hypothesis function the value of 𝑏₀, also called the **intercept**, shows the point where the estimated regression line crosses the 𝑦 axis. It’s the value of the estimated response 𝑓(𝑥) for 𝑥 = 0. The value of 𝑏₁ determines the **slope** of the estimated regression line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634f6ecb-9981-45b5-9936-6b73d63aa6c1",
   "metadata": {},
   "source": [
    "### Linear Regression with multiple varibale (many Features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeee7ec5-7cd6-46e1-ba0d-4613e4512f81",
   "metadata": {},
   "source": [
    "Linear regression with multiple variables is also known as **\"Multiple or Multivariate linear regression\"**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4863035c-d03a-4c85-9d99-a99f6f786172",
   "metadata": {},
   "source": [
    "Yet we have two parameter but what happened if many more parameter?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0157cc93-c238-4194-80f2-da41974efb79",
   "metadata": {},
   "source": [
    "- Hteta(x1, x2, x3, ..., xn)\n",
    "- n: numbers of feature\n",
    "- x(i): input (features) of i*th* training example\n",
    "- x(i)j: value of feature j in i*th* training example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ca4e21-55c4-4324-af0e-6d218ee64601",
   "metadata": {},
   "source": [
    "𝑓(𝑥₁, …, 𝑥ᵣ) = 𝑏₀ + 𝑏₁𝑥₁ + ⋯ +𝑏ᵣ𝑥ᵣ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5826f199-3457-476a-837b-b2843a894454",
   "metadata": {},
   "source": [
    "<img src=\"./images/1.png\" width=900px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f93bb58-a520-4fc3-a641-7b2b589120b3",
   "metadata": {},
   "source": [
    "Now you can Implememnt gradient descent or another tecnique to find Hypothesis parameter\n",
    "\n",
    "```suedo\n",
    "repeat until convergence{\n",
    "    tetai := tetai - alpha d/dtetaiJ(teta1, teta0)\n",
    "   } / i=[0, ..., n]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda16fdf-465d-4487-ab50-17d7e4d9bc5d",
   "metadata": {},
   "source": [
    "### Ploynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57b6f20-8877-4196-b335-e797c39f7651",
   "metadata": {},
   "source": [
    "In other words, in addition to linear terms like 𝑏₁𝑥₁, your regression function 𝑓 can include nonlinear terms such as 𝑏₂𝑥₁², 𝑏₃𝑥₁³, or even 𝑏₄𝑥₁𝑥₂, 𝑏₅𝑥₁²𝑥₂."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eaf6174-fba9-4970-ba35-63e8b6d67ecf",
   "metadata": {},
   "source": [
    "The simplest example of polynomial regression has a single independent variable, and the estimated regression function is a polynomial of degree two: 𝑓(𝑥) = 𝑏₀ + 𝑏₁𝑥 + 𝑏₂𝑥². Now, remember that you want to calculate 𝑏₀, 𝑏₁, and 𝑏₂ to minimize SSR. These are your unknowns!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c745207-e82e-4694-8e89-94917de07fd2",
   "metadata": {},
   "source": [
    "In the case of two variables and the polynomial of degree two, the regression function has this form: 𝑓(𝑥₁, 𝑥₂) = 𝑏₀ + 𝑏₁𝑥₁ + 𝑏₂𝑥₂ + 𝑏₃𝑥₁² + 𝑏₄𝑥₁𝑥₂ + 𝑏₅𝑥₂². The procedure for solving the problem is identical to the previous case. You apply linear regression for five inputs: 𝑥₁, 𝑥₂, 𝑥₁², 𝑥₁𝑥₂, and 𝑥₂². As the result of regression, you get the values of six weights that minimize SSR: 𝑏₀, 𝑏₁, 𝑏₂, 𝑏₃, 𝑏₄, and 𝑏₅."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13d9bc4-c71c-4e1e-84ec-9de99a894b3a",
   "metadata": {},
   "source": [
    "## Gradient Descent In Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38569002-b887-4c5c-96d8-16ba1921a657",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454cb15c-820d-460c-bea8-c54270ba13d5",
   "metadata": {},
   "source": [
    "In multivariae Linear Regression if size of feature not scalled, the speed of convergence be small."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a9d005-9ffb-4fdf-8953-791f117f97c9",
   "metadata": {},
   "source": [
    "<img src='./images/feature_scaling.png' width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843e4b31-4459-4686-9274-d72cd575cc50",
   "metadata": {},
   "source": [
    "Example: The above shown picture is a housing price example, where ‘x1’ is the size of the house and ‘x2’ refers to the number of bedrooms. The contours of the cost-function J(θ0,θ1)are elliptical-shaped and extremely skewed as shown initially. This may cause the gradient descent to converge to the global minimum in a very long time.\n",
    "\n",
    "However, if we scale the features, i.e. divide x1 by 2000 and divide x2 by 5and then plot the cost-function, the contours may look much more like circles. This provides a more direct path for gradient descent, which had a very complicated trajectory as compared to the initial plot of the cost function with unscaled features.\n",
    "\n",
    "            Notice that features rnages must convert to range -1< x <1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d508dcf-948d-4eac-9ec6-89419816a4ff",
   "metadata": {},
   "source": [
    ">range between `-3< x <3` and `\n",
    "-1/3< x < 1/3` is good but upon of these range raise computation time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207d48b1-028c-447f-a64c-c48ea344ea43",
   "metadata": {},
   "source": [
    "Alternative for Feature Scaling\n",
    "\n",
    "Mean normalization method can be used as an alternative for feature scaling. In this method we can scale up the size as (size — mean size) divided by the range of the size.\n",
    "\n",
    "Example: In the previous case x1 (size of the house) can be scaled as x1 = (size — 1000)/2000. This modifies the range of x1 as: -0.5<x1<+0.5\n",
    "\n",
    "This is how feature scaling and mean normalization method can be used to speed up the gradient descent process, by having the values of the input variables, more or less in the same range."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572112c9-61f8-4de0-8f2d-1ddb0c61ba3d",
   "metadata": {},
   "source": [
    "        in this way all feature value must replace with (xi - Meani) then apply this formula:\n",
    "        \n",
    "            xi - Meani / (range or SD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a81a7d-64e9-470f-8ab1-eeda6220d10b",
   "metadata": {},
   "source": [
    "### Learnig rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d424fda-522f-4e52-9f73-b98b7b96d4e5",
   "metadata": {},
   "source": [
    "- debuging:\n",
    "\n",
    "    How satisfy that no bug in our gradient descent algortihm?\n",
    "    plot Cost Function and attach x axis to *Number of Iterations* and y axis *minmum value of Cost function*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cb2b8f-859f-4393-8dea-547127c4dc8f",
   "metadata": {},
   "source": [
    "<img src=\"./images/learning_rate.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d2bb90-6b17-4358-959f-dbc61adea13d",
   "metadata": {},
   "source": [
    "if you plot then function is raised thus algorithm not work correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7287d07a-65a8-4f68-a8a6-5c700c3759ad",
   "metadata": {},
   "source": [
    "how to choose learning rate?\n",
    "\n",
    "<img src=\"./images/choose_learning_rate.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
